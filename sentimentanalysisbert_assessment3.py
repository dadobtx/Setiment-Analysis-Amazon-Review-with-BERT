# -*- coding: utf-8 -*-
"""SentimentAnalysisBERT_Assessment3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11wTgyHUGA4wexTtOlyzcFcQ6_hmSiRoE

# SENTIMENT ANALYSIS BY BERT AND AMAZON DATA SET
The present assessment was developed with the reference code and guide Youtube video Análisis de sentimientos con BERT en Python (Tutorial) by codificandobits and the notebook Getting-Things-Done-with-Pytorch/08.sentiment-analysis-with-bert.ipynb by curiousily

The assessment was developed by Dario Chicaiza 00321774T, Jahnavi Choragudi A00042212, and Edwin Camacho A00034927

**Setup** 
Transformers is the necessary library to work with BERT
"""

!pip install transformers

from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup
import torch
import numpy as np
from sklearn.model_selection import train_test_split
from torch import nn, optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
from textwrap import wrap
from sklearn.metrics import confusion_matrix, classification_report
from collections import defaultdict
import pickle
import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt')

#Libraries to create graphs
import matplotlib.pyplot as plt
import seaborn as sns
from pylab import rcParams
import matplotlib.pyplot as plt
from matplotlib import rc
import torch.nn.functional as F

# Initialization 
RANDOM_SEED = 42 #
MAX_LEN = 200 # Text review size
BATCH_SIZE = 16
NCLASSES = 2 # because there are two type of labels: positive sentiment and negative sentiment. BERT learns to identify pos and neg comments

np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)

"""**Data Preparation**"""

# sorted_data_acl with all reviews file must be loaded in you own google drive
from google.colab import drive
drive.mount('/content/drive')

## Parse xml file
def Processing_XML_Reviews_Dictionnaire(ListReviews):
    count = 0
    Review = []
    Reviews = {}
    for i in range(len(ListReviews)):
        if ListReviews[i] != '</review>\n':
            if ListReviews[i] == '<review>\n' and ListReviews[i+1] == '<unique_id>\n':
                # unique_id
                Review.append('unique_id/'+ListReviews[i+2])
            #if ListReviews[i] == '</unique_id>\n' and ListReviews[i+1] == '<unique_id>\n':
                #unique_idN
                #Review.append('unique_id/'+ListReviews[i+2])
            if  ListReviews[i] == '<asin>\n':
                #asin
                Review.append('asin/'+ListReviews[i+1])    
            if  ListReviews[i] == '<product_name>\n':
                #productName
                Review.append('product_name/'+ListReviews[i+1])
            #if  ListReviews[i] == '</product_type>\n' and ListReviews[i+1] == '<product_type>\n' :
                #here we append the producttype
                #Review.append('product_type/'+ListReviews[i+2])
            if ListReviews[i] == '<helpful>\n':
                #helpful
                Review.append('helpful/'+ListReviews[i+1])
            if ListReviews[i] == '<rating>\n':
                Review.append('rating/'+ListReviews[i+1])
            if ListReviews[i] == '<title>\n':
                Review.append('title/'+ListReviews[i+1])
            if ListReviews[i] == '<date>\n':
                Review.append('date/'+ListReviews[i+1])
            if ListReviews[i] == '<reviewer>\n':
                Review.append('reviewer/'+ListReviews[i+1])
            if ListReviews[i] == '<reviewer_location>\n':
                Review.append('reviewer_location/'+ListReviews[i+1])
            if ListReviews[i] == '<review_text>\n':
                Review.append('review_text/'+ListReviews[i+1])
        elif ListReviews[i] == '</review>\n':
            count = count + 1
            r = 'review'+ str(count) 
            Reviews[r] = Review
            #nfargou list
            Review = []
    return Reviews

##Convert reviews from Dictionary to pandas dataframe
def Porcessing_Dictonnary_ToDataFrame(Dict):
    #on prepare notre dataframe pour les données
    df = pd.DataFrame(columns=['unique_id','asin','product_name','helpful','rating','title',
                    'date','reviewer','reviewer_location','review_text'])
    count = 0
    for i,k in Dict.items():
        df.loc[count] = [k[0].split("/")[1].split("\n")[0],k[1].split("/")[1].split("\n")[0]
                              ,k[2].split("/")[1].split("\n")[0],k[3].split("/")[1].split("\n")[0]
                              ,k[4].split("/")[1].split("\n")[0],k[5].split("/")[1].split("\n")[0]
                              ,k[6].split("/")[1].split("\n")[0],k[7].split("/")[1].split("\n")[0]
                              ,k[8].split("/")[1].split("\n")[0],k[9].split("/")[1].split("\n")[0]
        ]
        count = count + 1

    return df

## Bring the data of the 4 type of products within the dataset into 2 dictionaries
import os
indir = '/content/drive/MyDrive/sorted_data_acl' #Copy the review file path in your google drive
PositifReviews = []
NegativeReviews = []
for root, dirs, filenames in os.walk(indir):
    for f in filenames:
        if f == "positive.review":
            log = open(os.path.join(root, f), 'r').readlines()
            PositifReviews.append(Processing_XML_Reviews_Dictionnaire(log))
            print(os.path.join(root, f))
        if f == "negative.review":
            log = open(os.path.join(root, f), 'r').readlines()
            NegativeReviews.append(Processing_XML_Reviews_Dictionnaire(log))
            print(os.path.join(root, f))

## datasets sorted by positive and negative
df_Postive_Rev = pd.DataFrame(columns=['unique_id','asin','product_name','helpful','rating','title',
                    'date','reviewer','reviewer_location','review_text'])
df_Negative_Rev = pd.DataFrame(columns=['unique_id','asin','product_name','helpful','rating','title',
                    'date','reviewer','reviewer_location','review_text'])
df = pd.DataFrame()

for Rev in PositifReviews:
    df = Porcessing_Dictonnary_ToDataFrame(Rev)
    df_Postive_Rev = pd.concat([df_Postive_Rev, df])
    df.empty

df1 = pd.DataFrame()
for Rev in NegativeReviews:
    df1 = Porcessing_Dictonnary_ToDataFrame(Rev)
    df_Negative_Rev = pd.concat([df_Negative_Rev, df1])
    df1.empty

df.shape

## Concatenate the data in 1 file
df_Postive_Rev['Class'] = "pos" # review positives
df_Negative_Rev['Class'] = "neg" #review Negatif
Reviews  = pd.concat([df_Postive_Rev,df_Negative_Rev])
Reviews.drop('unique_id',axis=1,inplace=True)  ## Unique_id Has the same information as "asin" "title" "reviewer" so it could be eliminated
test_Revs = pd.DataFrame(Reviews)

test_Revs.shape

## Count number of words in the text_review column
def identify_tokens(row):
    
    Clean_reviews = row['review_text']
    tokens = nltk.word_tokenize(Clean_reviews)
    # taken only words and numbers (not punctuation)
    token_words = [w for w in tokens if w.isalnum()]
    return token_words

test_Revs['words'] = test_Revs.apply(identify_tokens, axis=1)


## Count words per row
def total_elements(row):
  words = row['words']
  count = 0
  for element in words:
      count += 1
  return count

test_Revs['num_words'] = test_Revs.apply(total_elements, axis=1)

# Delete null or short word row indexes from dataFrame
indexNames = test_Revs[test_Revs['num_words'] <= 1 ].index
test_Revs.drop(indexNames , inplace=True)

test_Revs.columns

df=test_Revs

df.columns

#Clean and crop columms we are not going to use
df.drop(['asin', 'product_name', 'helpful', 'rating', 'title', 'date',
       'reviewer', 'reviewer_location', 'words',
       'num_words'],axis=1,inplace=True)
df.rename(columns={'Class':'sentiment','review_text':'review'},
               inplace=True)
df

test_Revs.to_csv('reviewsFull.csv')

df=pd.read_csv('reviewsFull.csv')
print(df.shape)

# Re-setup dataset
df['label'] = (df['sentiment']=='pos').astype(int)
df.drop('sentiment', axis=1, inplace=True)
df.head()

# How balanced is the dataframe between pos(1) and negative(0) reviews
sns.countplot(df.label)
plt.xlabel('Sentiment');

"""**TOKENIZATION**"""

PRE_TRAINED_MODEL_NAME = 'bert-base-cased'
tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)

"""**DATA SET CREATION**"""

# Class to create DATA SET

class IMDBDataset(Dataset):

  def __init__(self,reviews,labels,tokenizer,max_len):
    self.reviews = reviews
    self.labels = labels
    self.tokenizer = tokenizer
    self.max_len = max_len

  def __len__(self):
      return len(self.reviews)
   # get ramdonly data (batchsize)  
  def __getitem__(self, item):
    review = str(self.reviews[item])
    label = self.labels[item]
    encoding = tokenizer.encode_plus(
        review,
        max_length = self.max_len,
        truncation = True,
        add_special_tokens = True,
        return_token_type_ids = False,
        pad_to_max_length = True,
        return_attention_mask = True,
        return_tensors = 'pt'
        )
    #get back a dictionary

    return {
          'review': review,
          'input_ids': encoding['input_ids'].flatten(),
          'attention_mask': encoding['attention_mask'].flatten(),
          'label': torch.tensor(label, dtype=torch.long)
      }

# Data loader:

def data_loader(df, tokenizer, max_len, batch_size):
  dataset = IMDBDataset(
      reviews = df.review.to_numpy(),
      labels = df.label.to_numpy(),
      tokenizer = tokenizer,
      max_len = MAX_LEN
  )

  return DataLoader(dataset, batch_size = BATCH_SIZE, num_workers = 4)

"""**SPLIT DATA TRAIN AND DATA TEST**"""

df_train, df_test = train_test_split(df,test_size = 0.2, random_state=RANDOM_SEED)

train_data_loader = data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)
test_data_loader = data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)

"""**MODELING BERT CLASS AND FINE-TUNED THE PRE-TRAINED BERT MODEL**"""

# Class to Model the BERT and fine-tuning adding the last custom layer to retrain the pre-trained BERT Model with our Amazon dataset

class BERTSentimentClassifier(nn.Module):

  def __init__(self, n_classes):
    super(BERTSentimentClassifier, self).__init__()
    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME,return_dict=False)
    # Reduce and improve overfitting. Train data and test data generate a similar accuracy
    self.drop = nn.Dropout(p=0.3)
    #hidden size 768 BERT model
    self.linear = nn.Linear(self.bert.config.hidden_size, n_classes)
    

  
  def forward(self, input_ids, attention_mask):
    _, cls_output = self.bert( #cls_output = clasificate token
        input_ids = input_ids,
        attention_mask = attention_mask
    )
    drop_output = self.drop(cls_output)
    output = self.linear(drop_output)
    return output

# Initialize BERT Class
model = BERTSentimentClassifier(NCLASSES)
model = model.to(device)

"""**TRAINING**"""

EPOCHS = 3
optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)
total_steps = len(train_data_loader) * EPOCHS
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps = 0,
    num_training_steps = total_steps
)
loss_fn = nn.CrossEntropyLoss().to(device)

# Function to train the model

def train_model(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):
  model = model.train()
  losses = []
  correct_predictions = 0
  for batch in data_loader:
    input_ids = batch['input_ids'].to(device)
    attention_mask = batch['attention_mask'].to(device)
    labels = batch['label'].to(device)
    outputs = model(input_ids = input_ids, attention_mask = attention_mask)
    _, preds = torch.max(outputs, dim=1)
    loss = loss_fn(outputs, labels)
    correct_predictions += torch.sum(preds == labels)
    losses.append(loss.item())
    #back propagation
    loss.backward()
    # Avoid the gradient encreases too much and train stalls
    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    optimizer.step()
    scheduler.step()
    optimizer.zero_grad()
  return correct_predictions.double()/n_examples, np.mean(losses)

# Function to eval the model

def eval_model(model, data_loader, loss_fn, device, n_examples):
  model = model.eval()
  losses = []
  correct_predictions = 0
  with torch.no_grad():
    for batch in data_loader:
      input_ids = batch['input_ids'].to(device)
      attention_mask = batch['attention_mask'].to(device)
      labels = batch['label'].to(device)
      outputs = model(input_ids = input_ids, attention_mask = attention_mask)
      _, preds = torch.max(outputs, dim=1)
      loss = loss_fn(outputs, labels)
      correct_predictions += torch.sum(preds == labels)
      losses.append(loss.item())
  return correct_predictions.double()/n_examples, np.mean(losses)

# Run training iteration according the given epochs

history = defaultdict(list)
for epoch in range(EPOCHS):
  print('Epoch {} de {}'.format(epoch+1, EPOCHS))
  print('------------------')
  train_acc, train_loss = train_model(
      model, train_data_loader, loss_fn, optimizer, device, scheduler, len(df_train)
  )
  test_acc, test_loss = eval_model(
      model, test_data_loader, loss_fn, device, len(df_test)
  )
  print('Training: Loss: {}, accuracy: {}'.format(train_loss, train_acc))
  print('Evaluation: Loss: {}, accuracy: {}'.format(test_loss, test_acc))
  print('')
  history['train_acc'].append(train_acc)
  history['train_loss'].append(train_loss)
  history['test_acc'].append(test_acc)
  history['test_loss'].append(test_loss)
  
  

# save the model to disk
filename = 'TrainedModelCPU1_bert_.sav'
pickle.dump(model, open(filename, 'wb'))

"""**GRAPH THE OUTCOMES**"""

sns.set(style='whitegrid', palette='muted', font_scale=1.2)
HAPPY_COLORS_PALETTE = ["#01BEFE", "#FFDD00", "#FF7D00", "#FF006D", "#ADFF02", "#8F00FF"]
sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))
rcParams['figure.figsize'] = 12, 8

#Graph the Train accuracy Vs. Test accuracy
plt.plot(history['train_acc'], label='train accuracy')
plt.plot(history['test_acc'], label='Test accuracy')

plt.title('Training history')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend()
plt.ylim([0, 1]);

def get_predictions(model, data_loader):
  model = model.eval()
  
  review_texts = []
  predictions = []
  prediction_probs = []
  real_values = []

  with torch.no_grad():
    for d in data_loader:

      texts = d["review"]
      input_ids = d["input_ids"].to(device)
      attention_mask = d["attention_mask"].to(device)
      targets = d["label"].to(device)

      outputs = model(
        input_ids=input_ids,
        attention_mask=attention_mask
      )
      _, preds = torch.max(outputs, dim=1)

      probs = F.softmax(outputs, dim=1)

      review_texts.extend(texts)
      predictions.extend(preds)
      prediction_probs.extend(probs)
      real_values.extend(targets)

  predictions = torch.stack(predictions).cpu()
  prediction_probs = torch.stack(prediction_probs).cpu()
  real_values = torch.stack(real_values).cpu()
  return review_texts, predictions, prediction_probs, real_values

y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(
  model,
  test_data_loader
)
class_names = ['negative', 'positive']

#Graph confusion matrix
def show_confusion_matrix(confusion_matrix):
  hmap = sns.heatmap(confusion_matrix, annot=True, fmt="d", cmap="Blues")
  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')
  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')
  plt.ylabel('True sentiment')
  plt.xlabel('Predicted sentiment');

cm = confusion_matrix(y_test, y_pred)
df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)
show_confusion_matrix(df_cm)

"""**TEST THE MODEL**"""

def classifySentiment(review_text,model):
  encoding_review = tokenizer.encode_plus(
      review_text,
      max_length = MAX_LEN,
      truncation = True,
      add_special_tokens = True,
      return_token_type_ids = False,
      pad_to_max_length = True,
      return_attention_mask = True,
      return_tensors = 'pt'
      )
  
  input_ids = encoding_review['input_ids'].to(device)
  attention_mask = encoding_review['attention_mask'].to(device)
  output = model(input_ids, attention_mask)
  _, prediction = torch.max(output, dim=1)
  print("\n".join(wrap(review_text)))
  if prediction:
    print('Sentiment: Positive')
  else:
    print('Sentiment: Negative')

review_text = "I'm not sure why Sony, which now owns I Dream of Jeannie, decided to colorize the first season of this series.  Whatever the reason, you can readily tell by looking at the prices here on Amazon.com that the original black-and-white version of the first season is worth a lot more.  The reason for that is simple--I Dream of Jeannie was originally broadcast in black-and-white.  And for a television fan like myself, that's the ONLY way to watch the first season."

classifySentiment(review_text,model)